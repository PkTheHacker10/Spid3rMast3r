#!/bin/python3
import os
import sys
from subprocess import run

all_urls=[]
waybackurls=[]
gau_urls=[]
katana_urls=[]
gospider_urls=[]
result_folder="webcrowlings"
all_urls_location=f"{result_folder}/all_urls.txt"

def waybackurls_crawler(all_subdomains):
    try:
        for domain in all_subdomains:
            if domain.startswith("http://") or domain.startswith("https://"):
                domain=domain.split("//")[1].strip()
                
            print(f"\tEnumerating : {domain}")
            output=run(f"waybackurls {domain}",capture_output=True,text=True,shell=True)
            waybackurls.append(output.stdout)

        with open("webcrowlings/waybackurls.txt","w") as opened_file:
            for url in waybackurls:
                opened_file.write(url.strip() + "\n")

    except Exception as E:
        print(f"[ + ] Exception (katana): {E}")

    print("[ + ] output saved as \"webcrowlings/waybackurls.txt\"") 

def gau_crawler(all_subdomains):
    try:
        for domain in all_subdomains:
            if domain.startswith("http://") or domain.startswith("https://"):
                domain=domain.split("//")[1].strip()
                
            print(f"\tEnumerating : {domain}")
            output=run(f"gau {domain}",capture_output=True,text=True,shell=True)
            gau_urls.append(output.stdout)

        with open("webcrowlings/gau.txt","w") as opened_file:
            for url in gau_urls:
                opened_file.write(url.strip())

    except Exception as E:
        print(f"[ + ] Exception (gau): {E}")

    print("[ + ] output saved as \"webcrowlings/gau.txt\"") 

def katana_crawler(all_subdomains):
    try:
        for domain in all_subdomains:
            if domain.startswith("http://") or domain.startswith("https://"):
                domain=domain.split("//")[1].strip()

            print(f"\tEnumerating : {domain}")
            output=run(f"katana -silent -u {domain}",capture_output=True,text=True,shell=True)
            katana_urls.append(output.stdout)

        with open("webcrowlings/katana.txt","w") as opened_file:
            for url in katana_urls:
                opened_file.write(url.strip())

    except Exception as E:
        print(f"[ + ] Exception (katana): {E}")
    
    print("[ + ] output saved as \"webcrowlings/katana.txt\"") 

def gospider_crawler(all_subdomains):
    try:
        for domain in all_subdomains:
            if not domain.startswith("http://") or domain.startswith("https://"):
                domain="https://" + domain

            print(f"\tEnumerating : {domain}")
            output=run(f"gospider --site {domain} -a",capture_output=True,text=True,shell=True)
            gospider_urls.append(output.stdout)

        with open("webcrowlings/gospider.txt","w") as opened_file:
            for url in gospider_urls:
                opened_file.write(url.strip())

    except Exception as E:
        print(f"[ + ] Exception (gospider): {E}")
    
    print("[ + ] output saved as \"webcrowlings/gospider.txt\"") 

def save_all_urls():
    try:
        with open(all_urls_location,"w") as opened_file:
            for wayback_url in waybackurls:
                opened_file.write(wayback_url.strip())
                
            for gau_url in gau_urls:
                opened_file.write(gau_url.strip())

            for katana_url in katana_urls:
                opened_file.write(katana_url)
                
    except Exception as E:
        print(f"[ + ] Exception (katana): {E}")

        print("[ + ] Enumeration Completed ,All urls :\" webcrawling/all_urls.txt\"")


def main():
    if len(sys.argv)>=2:
        file_path=sys.argv[1]
        try:
            try:
                with open(file_path,"r") as opened_file:
                    all_subdomains = [line.strip() for line in opened_file]
            
            except FileNotFoundError:
                print(f"File \"{file_path}\" not found")
                exit(1)

            # Creating result folder
            os.makedirs("webcrowlings",exist_ok=True)

            print("[ + ] Waybackurls crawling started\n")
            #waybackurls_crawler(all_subdomains)

            print("[ + ] gau crawling started")
            #gau_crawler(all_subdomains)

            print("[ + ] katana crawling started")
            #katana_crawler(all_subdomains)

            print("[ + ] gospider crawling started")
            gospider_crawler(all_subdomains)

            # Saving all urls in webcrowlings/all_urls.txt
            print("[ + ] Saving All tools urls to webcrowling/all_urls.txt")
            #save_all_urls()

        except Exception as E:
            print(f"[ + ] Exception : {E}")

    else:
        print("[ + ] Usage : url_spider.py <domain_list>")
        exit(1)

if __name__ == "__main__":
    main()

